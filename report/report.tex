%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% General TODO:
%  - fix images' size
%  - improve style of code/identifier
%  - references / bibliography
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass{article}

\usepackage{graphicx}
\usepackage[usenames,dvipsnames,svgnames,table]{xcolor}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{titlesec}
\usepackage{cite}
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{mathtools}
\usepackage{tabularx}  % for 'tabularx' environment and 'X' column type
\usepackage{ragged2e}  % for '\RaggedRight' macro (allows hyphenation)
\newcolumntype{Y}{>{\RaggedRight\arraybackslash}X}
\usepackage{booktabs}  % for \toprule, \midrule, and \bottomrule macros

\newcommand{\sectionbreak}{\clearpage}

%%%
% Commands

\newcommand{\ID}[1]{\mbox{\textsc{#1}}}
\newcommand{\PATH}[1]{\mbox{\texttt{#1}}}

\newcommand{\TODO}[1]{\texttt{\textcolor{YellowOrange}{(#1)}}} % for inline TODO
\newcommand{\REST}[1]{\textsf{#1}}

\newcommand{\tablewidth}{0.9\linewidth}

%
%%%

\title{Large-scale Information Extraction from Neuroscientific Literature}

\date{Spring 2015}

\author{Marco Antognini}

\begin{document}

\maketitle

\begin{center}
    Master Semester Project under the supervision of\\
    Jean-Cédric Chappelier \& Renaud Richardet\\
    Artificial Intelligence Laboratory LIA - EPFL
\end{center}

\pagenumbering{gobble}
\newpage
\pagenumbering{arabic}


\tableofcontents

\section{Introduction}

The neuroscientists of Blue Brain Project \cite{bbp}, in their quest to understand how the
brain works, have to understand the purpose of the different components of the brain such as cells
or whole brain regions and connect their effects with other parts of the brain or other organs.
However, since the amount of information is colossal and is obviously not memorisable by individual
human being or even by a team of researchers, tools need to be created in order to extract the relevant
data from the scientific literature in a precise and fast manner.

Sherlok \cite{sherlok} was created to solve this large-scale data mining problem. Based on a
Representational State Transfer (REST) API , Sherlok is a service capable of extracting information
relevant to specific situation directly from a given text. It is designed to be user-friendly and
flexible so that it can analyse, via standard REST requests, any kind of input text formats,
provided that there is an adaptor to extract the actual text, and pull out the interesting metadata
for the user.

Text adaptors can extract more than just the raw text. For example, a PDF extractor could include
information about text emphasis, whether a word is part of a section title, if proper name is an
author or replace images by a text description.

Currently, the main dependency of Sherlok is, but is not restricted to, bluima \cite{bluima}, which
is an open source project developed by the Blue Brain Project and consists of a large collection of
UIMA-based \emph{annotators} (also called \emph{engines}) \cite{uima} that generate metadata from
the input text. We will focus on those specific annotators for this project.

Metadata is defined by a begin and end position in the raw text, a type and in some cases extra
attributes. It can be as simple as representing the boundaries of sentences to more complex notions
such as brain regions.

For example, annotating the following sentence for species and brain regions, generates the
following annotations (in addition to the part of speech tags (PoS) that are not shown here).

\[
    \text{Sex differences in the gross size of the}
    \overbrace{\text{rat}}^{\mathclap{\text{Species}}}
    \underbrace{\text{neocortex}}_{\mathclap{\text{Brain Region}}} \text{.}
\]

Figure \ref{fig:sherlok_basic_rest_call} depicts how a REST query is handled from a very high level
point of view by Sherlok.

\begin{figure}
    \centering
    \includegraphics[width=200pt]{res/sherlok_basic_rest_call.png}
    \caption{Part of Speech Pipeline}
    \label{fig:sherlok_basic_rest_call}
\end{figure}

While each annotator works on its own to generate metadata, Sherlok let the user combines them in
\emph{pipelines} where the output of one engine is used as the input for the next one. And in order
to reuse engines in different pipelines, Sherlok separates their definitions: engines are first
grouped in bundles, which make the bridge between Sherlok's scripting system and the UIMA annotators
written in Java, and then they can be used by several pipelines.

The main challenge of this project was to separate the algorithms used in the engines from their
resources, which could be as simple as a list of countries to trained models to detect part of
speech, in bluima in order to add flexibility to the installation, update and usage of those
resources in Sherlok.  Initially, bluima relied on a system-wide setting to locate its resources.
However, since we wanted to make Sherlok run on as many systems as possible -- which should include
a server architecture where a dedicated machine is in charge of storage -- we couldn't rely on this
property any longer.  Hence, the first and main task of this project was to devise a system capable
of working without hard coded paths.

It was also decided that an clever system for installing and loading resources on demand (i.e.
when loading a pipeline) should be introduced in Sherlok. And, finally, Sherlok should at least have
a full brain regions NER and connections pipeline based on bluima tools, which includes sentence
detection, part of speech tagging, species recognition, measures and units extraction and more.

\TODO{include quick plan of paper}

\section{Sherlok}

Sherlok is an open source driven service for text information extraction initiated by Renaud
Richardet when working on the Blue Brain Project to extend and popularise bluima engines. However,
Sherlok is not restricted to neuroscience and as such, system administrators don't require any prior
neuroscientific knowledge.

\subsection{REST-Based Service}

The service provided through a classic Representational State Transfer (REST) API can be installed
in-house by system administrators to match the specific needs of researchers. From the user
perspective, only a basic understanding of the Hypertext Transfer Protocol (HTTP) is required to
build queries used to communicate with Sherlok.

\TODO{insert table of queries}

While its implementation is written in Java, any language or tool with basic socket support can
communicate with Sherlok and benefits from its capabilities without going through the frequent
required hassle to build a C interface to communicate between Java and another language.
Additionally, as a proof of concept, Sherlok provides a JavaScript, Java and Python client API to
emit the standard REST queries in those languages. And due to the nature of REST and HTTP protocols
they can easily be extended to support more languages.

From a regular user point of view, \REST{/annotate/<pipeline~name>?text=<input~text>} is all there is to
know to use a given pipeline to process some text: nothing is installed on the user side and
everything is directly available without investing in powerful personal computer. And if the
existing pipelines don't fit the user's needs a new pipeline can be submitted to Sherlok which will
automatically install the required dependencies, would they be Java packages or regular files needed
at runtime.Additionally, Sherlok comes with a web editor to easily install new pipelines, edit them
and process text without manually writing any REST queries.

From the system administrator point of view, Sherlok is no less flexible or harder to use. The
installation procedure is as simple as extracting an archive and launching an executable. And
because Sherlok runs on the Java Virtual Machine (JVM), any system with a proper Java Runtime
Environment 7 (JRE7) should be able to run Sherlok. The memory and disk requirement depends
on the pipelines that the users will run. However, the system administrator doesn't have to play
resources for pipelines that are not used: Sherlok easily allows obsolete pipelines or resources to
be deleted.

Were one server not be capable of providing enough computational power to handle all users'
requests, several instance of Sherlok can be run in a \emph{slave} mode to balance the workload on
several machines.

Moreover, if the system administrator wants to restrict the installation or modification of
pipelines, he can run Sherlok in \emph{sealed} mode to freeze the configuration of the service.
While Sherlok doesn't have any particular Access Control List (ACL) mechanism to support
fine-grained access control and restrict users permissions on a case by case basis, custom ACL
system can easily be built on top of Sherlok: a simple proxy program can integrate restricted access
using for example Unix accounts or a Lightweight Directory Access Protocol (LDAP) server to filter
requests before Sherlok can process them.

\subsection{UIMA-Based Configurable Service}

Internally, Sherlok will process the input text through Unstructured Information Management
Architecture (UIMA) \cite{uima} engines or more generally through a Rule-based Text Annotation
(RUTA) scripts \cite{ruta}. Both RUTA and UIMA are projects of the Apache Software Foundation
\cite{apachefundation} focusing in analysing large volumes of unstructured information in order to
discover knowledge that is relevant to an end user. While Sherlok focuses on the Java Frameworks for
UIMA, it could be extended in the future to also support the C++ or Scaleout Frameworks.

In order to let users who are not Java developers combine engines together, Sherlok has a Java
agnostic configuration system that separate the definition of engines from their usage in pipelines.

\begin{description}
    \item[Bundles] The user lists the UIMA engines that he wants to use in \emph{bundles} by specifying
        the Java package dependencies, the required resource files and configure the different
        parameters of the engines. To simplify installing resources, Sherlok supports download at
        execution time of Git repositories or HTTP(S) URLs, but also lets the user specify local
        path on the server if needed.
    \item[Pipelines] The user writes the RUTA script which can be as simple as chaining engines from
        the bundles configuration files in \emph{pipelines}. The whole features set of RUTA is
        available to the user so that simple scripts don't require any Java implementation from the
        user.
\end{description}

By default Sherlok comes with pipelines to extract information about brain regions, part of speech
and much more. The engines used in those pipelines come from bluima, which is presented in Section
\ref{sec:bluima}.

The results of annotation are presented to the used in JSON format \cite{json}. For example, the
\ID{bluima.sentence} pipeline will give the following output when processing ``\textit{Switzerland
is close to Italy, the land of pasta and Michelangelo. Paris is in France. And Madrid in Spain.}'':

\TODO{insert JSON output listing}

For quality purpose, Sherlok also allows the user to provide tests for pipelines that can be run
through the \REST{/test/<pipeline name>} query in order to ensure that pipelines continue to work
properly after updating Sherlok or the engines' dependencies.

\section{Bluima, UIMA and RUTA}
\label{sec:bluima}

Bluima is an augmented collection of different libraries and UIMA \cite{uima} engines open-sourcly
developed that the Blue Brain Project \cite{bbp} decided to aggregate together to simplify
large-scale information extraction from the neuroscientific literature. It includes for example a
species annotator based on the Linnaeus library \cite{linnaeus}, or the OpenNLP toolkit
\cite{opennlp} to perform Part of Speech (PoS) tagging, sentence splitting and more. The complete
list of subproject is detailed in Table \ref{tab:bluima_subprojects}.

\begin{table}[h]
    \centering
    \begin{tabularx}{\tablewidth}{@{} c Y @{}} % no margin at extremum
        \toprule

        OpenNLP \cite{opennlp} & a machine learning based toolkit for the processing of natural
        language text \\

        \midrule

        Linnaeus \cite{linnaeus} & a general-purpose dictionary matching software, capable of
        processing multiple types of document formats in the biomedical domain \\

        \midrule

        & \TODO{add the others} \\

        \bottomrule
    \end{tabularx}
    \caption{bluima aggregated projects}
    \label{tab:bluima_subprojects}
\end{table}

\TODO{explain how UIMA works}

\TODO{explain what RUTA is}

\TODO{explain how Sherlok use that}



\section{Packaging: a first approach that didn't work}
\label{sec:packaging1}

\TODO{find better title}

The initial idea was to base the system on Apache Maven \TODO{ref}. However, while implementing the strategy described below we realised that it was not as flexible as predicted. A better alternative is presented in Section \ref{sec:packaging2}. This section describes our first approach.

One of the first objective of this project was the disassociation of algorithms (engine annotators) and their different resources (model files, configuration files) and devise a packaging system, based on Apache Maven, that would be flexible while being simple and convenient to use and let the user define pipelines using specific annotators and models.

\subsection{Packaging}

\TODO{explain that packages would be released on public repo, and thus making them available}

\TODO{quickly present DKpro packaging strategy \& ant script. Say why we did not go with it.}

With Maven, main idea is to store a set of resources, such as dataset trained on a specific corpus of texts, in a jar file and access them in read-only mode at runtime. Package dependencies can be used to ensure that some files are available at runtime and ideally package version should be used to let the user update resources independently of the algorithms.

\TODO{Insert example, e.g. Sentence}

It might not be clearly apparent for someone not used to work with Java jar system, but with this approach we cannot modify the resource files at runtime since they have been packaged, and this for mainly two reasons. Firstly, the Java API doesn't provide standard tools to modify such archives. And secondly, if the jar archive were signed (which is the standard procedure) then modifying its content would actually break the signature and therefore corrupt the archive.

Additionally, the API to access resources inside jar files is not based on Java \ID{File} objects. Instead algorithms have to use \ID{InputStream} objects through \ID{ClassLoader} objects to access the content of a given file. We will discuss how the current code need to be refactored in Section \ref{sec:restructuring_bluima}.

Here we describe three alternatives that we used to contrast alternative strengths and weaknesses before concluding with a fourth variant that should match our needs. Below, \ID{ada} and \ID{bob} denote two variants of a kind of resource that are compatible with a common algorithm package, denoted \ID{algo}. The \ID{using*} packages can be thought as pipelines in the context of bluima/sherlok.

\subsubsection{Version A}

The first system we analysed spread each algorithm and set of resources into individual packages that depends on \ID{modeldep}, a proxy utility to access the resource files inside jar archives. If an algorithm depends on some resource file then it accepts as input parameter the model name (as a string) and uses the proxy to load the file. In this scenario pipeline packages have dependencies toward their annotators but also toward specific models as shown in Figure \ref{fig:pkgsysA}.

This system has several positive aspects. First of all, tests can be easily written by specifying potentially multiple resource packages as dependencies with Maven's test scope. Then, \ID{modeldep} also defines a clear Java interface to define the contract for models and their versions. Additionally, the proxy system centralises the utility to load resources and therefore eases maintenance by not involving code duplication in several packages. However, models could not be swapped on the fly since it would involve recompiling and repackaging \ID{using\_my\_algo}.  Alternatively, to prevent modifications, \ID{using\_my\_algo} could depend on both \ID{ada} and \ID{bob} packages but this would not be as flexible as we want it to be.

\begin{figure}
\centering
\includegraphics[width=200pt]{res/packaging_version_A.png}
\caption{Packaging Version A}
\label{fig:pkgsysA}
\end{figure}


\subsubsection{Version B}

The second approach is based on Maven's version string: an \ID{algo} will depend on an abstract \ID{model} which provide a mechanism to load a specific file from its jar archive and both \ID{ada} and \ID{bob} model versions are defined as subversion of \ID{model}. For example, the convention could be that if \ID{model} is defined as version \ID{0.1} then \ID{ada} will use the string \ID{0.1-ADA} to define its version and the \ID{algo} will accept versions in the range $ [0.1,0.2) $.

The model actually used will be either specified at compile time (cf. \ID{using.algo2}) or be selected at runtime depending on the installed packages (cf. \ID{using.algo1}) as depicted on Figure \ref{fig:pkgsysB}. While this offers a great flexibility to the user designing pipelines and allows him to swap models without repackaging anything, it means that test projects cannot ensure the correctness of more than one model version at a time. Moreover, if no specific version is bound to the pipeline, such as with \ID{using.algo1}, then there is no strong guarantee that a valid version is available at runtime.

\begin{figure}
\centering
\includegraphics[width=200pt]{res/packaging_version_B.png}
\caption{Packaging Version B}
\label{fig:pkgsysB}
\end{figure}


\subsubsection{Version C}

The third version we studied is the most simple one: as illustrated on Figure \ref{fig:pkgsysC}, instead of splitting everything in separate package, only annotators are packaged independently of pipelines and resources, which are grouped together. On the one hand this system couldn't be simpler but on the other, since pipelines often involve several engine annotators that are themselves used in several pipelines, the coupling of models and the corresponding algorithm implies that many packages have to be created to support each combination of models.


\begin{figure}
\centering
\includegraphics[width=200pt]{res/packaging_version_C.png}
\caption{Packaging Version C}
\label{fig:pkgsysC}
\end{figure}

\subsubsection{The verdict - Version D}

After exploring possibilities offered by the Maven packaging system we analysed how resources and engines are related to each others and used by pipelines in bluima. Figure \ref{fig:pkgsysD} depicts those relations. It was also considered that repackaging is an acceptable cost to swap models. The most important point was avoiding at all cost to package an exponentially huge number of pipelines to match each and every possible combinations and for this reason version C was discarded.

We also reflected on the structure of the algorithm, especially on their input arguments. We came to the conclusion that, mostly for flexibility, they should accept \ID{InputStream}s as input and not be bound to any models. Instead, the pipelines will be in charge to give them the proper resource data streams. Therefore the pipeline will have dependencies toward algorithms and models.

Finally, in order to centralise code and simplify loading resources we introduce \ID{ModelProxy}, a utility class used by pipelines that, given a \ID{String} representing a class name, opens a stream to a given file inside the class' jar archive.

In some cases, when processing the resource as a stream, it can be convenient to have access to its original filename (e.g. when reading a compressed archive). Therefore we encapsulate the name and the stream in a class -- \ID{ModelStream} -- that can be seen as a specialised \ID{InputStream} with a filename property.

\begin{figure}
\centering
\includegraphics[width=200pt]{res/packaging_version_D.png}
\caption{Packaging Version D}
\label{fig:pkgsysD}
\end{figure}


\subsection{Restructuring bluima}
\label{sec:restructuring_bluima}

\TODO{better explain strategy before details}

Now that the packaging strategy for annotators and their resources has been designed we can actually apply it. It was decided to start with a relatively easy annotator -- namely \ID{SentenceAnnotator} which is part of OpenNLP -- to make sure our previous decision was indeed viable. The approach was based on an iterative process that can be applied to other modules as well. However, let's first take a step back and discuss the file hierarchy and how the different modules are related to each others.

\subsubsection{File Hierarchy}

We wanted to create a structured, yet flexible, file hierarchy for the annotators and their
resources. Since we are using Maven, we based our design on parent-child relationship in the
\ID{pom.xml} files: the root pom file, in addition to actually loading its children, defines the
main settings, such as the Java version to use or the list of general dependencies, that will be
shared with every of its children. The root pom file references both modules \PATH{modules/pom.xml}
and \PATH{resources/pom.xml} that are responsible for loading their own children, that is the different annotators or utilities for the former and the actual resource for the latter.

It follows that when running the install (or test) command on the root Maven project then every algorithms and resources are installed (or tested).

\subsubsection{Case Study: Converting OpenNLP}

When converting an annotator such as \ID{SentenceAnnotator}, the first thing to do is to identify the unit test responsible for its quality \TODO{add details on unit test in §bluima/§sherlok}. In this case, the annotator was imported from UIMA \TODO{ref} and it was still based on the XML engine descriptor \TODO{ref} \TODO{add explanation/contrast between XML engine descriptor \& java code in §bluima}. Therefore, we first have to make it compatible with the new and compact pipeline runner from uimaFIT \TODO{ref}.

Once the unit tests are properly working, we move on to the generation of the resource packages. Since this phase is completely repetitive, we devised a script \TODO{ref to wiki} to make the developer live less cumbersome: given a resource file, a resource name and a few other technical details we can produce a resource package, as described previously, that is ready to be used with an annotator.

However, many annotators are currently using Java \ID{File} parameters, or Java \ID{String}s pointing to a local file. Therefore, we have to refactor these annotators and their dependencies in order to use \ID{ModelStream} instead. The following strategy was applied to convert PoS, Token and Chunk annotator of OpenNLP:

\begin{enumerate}

\item Renaming the \ID{PARAM\_MODEL\_FILE} parameter (or any similar properties) of the annotator into \ID{PARAM\_MODEL}.

\item Updating its \ID{initialize} method to load a \ID{ModelStream} through \ID{ModelProxy} and the \ID{PARAM\_MODEL} parameter.

\item Updating the annotator's dependencies such that they can be constructed from \ID{ModelStream}. Usually this plays well with the current implementation that must at some point use a \ID{FileInputStream}. Hence, we can plug in our custom stream in place of the regular \ID{FileInputStream} without restructuring the dependencies too much.

\item Updating the dependency list in the annotator's \ID{pom.xml} file to reflect any additional resource dependency.

\end{enumerate}

Finally, at this last point the unit test should be updated to use resource package instead of
absolute path to the resource file with \ID{PARAM\_MODEL}.

\subsection{Pipeline Configuration in Sherlok}

\TODO{explain how this gets configured in Sherlok}

\section{Packaging Strategy}
\label{sec:packaging2}

\subsection{Current Limitations}

With the strategy exposed in Section \ref{sec:packaging1}, every single resource file is packaged in a separate jar file and there is a one-to-one mapping between a file and a \ID{ModelResource}. Actually, several files can be grouped inside the same jar as long as the corresponding \ID{ModelResource} classes are also in the same jar. However, this has several limitations that were either not identified during the design process or wrongly not considered as significantly limiting for our use case. The purpose of this proposal is to fix those limitations by proposing a new packaging strategy for resources.

\subsubsection{Refactoring}

It has been decided that, with \ID{ModelResource}, we shall use \ID{InputStream} instead of
\ID{File} and therefore refactor some code in bluima. We initially guessed that this refactoring
would in practice not be a large issue because, at some point, we need to read the file -- using
\ID{FileInputStream} or alike -- and therefore could use a more abstract concept (\ID{InputStream}
vs \ID{File}) without much trouble. It is true that for simple engines this refactoring is not an
issue. Unfortunately, for more complex annotators this task is actually hard or even impossible.
This is particularly true for annotators using external code, where sometime \ID{File} or even
\ID{String} paths to resources are used. For example, the \ID{LinnaeusAnnotator} is an engine that wraps the external Linnaeus library. This library uses an \ID{ArgParser} that cannot be created from a data stream, but requires \TODO{XXX}. Hence, the only working solution that works without refactoring the (external) Linnaeus library is to extract the configuration file into a temporary directory and provide the path to this temporary directory to the Linnaeus library.

\subsubsection{Resource Interdependency}

Attempting to convert \ID{LinnaeusAnnotator} also highlighted another issue: its main configuration
file makes references to some other files with relative paths. The issue here is that with our resource packaging system the files are only accessible through streams via \ID{ModelProxy} but not paths.

Here there are two potential solutions:

\begin{itemize}

\item either we completely rewrite Linnaeus, since it is a closed source program, to support reference to \ID{ModelResource} in the configuration files;

\item or we put all the related configuration files in an archive -- say, a tar file -- and extract them into a temporary directory.

\end{itemize}

Obviously the first solution is not practical within the short time period allocated to this project and not sustainable in the long term, as it would require to fork the original library, if not restart from scratch, and constantly update the forked codebase. The second solution, however, could be achieved. While it isn't perfect, it actually solves (partially) a more general issue discussed below.

\subsection{Batch Of Files}

Like with \ID{LinnaeusAnnotator}, the \ID{BrainRegionPipes} uses a batch of files: a collection of configuration files used to set up an annotator or its dependencies. For the \ID{BrainRegionPipes} the problem is easier since no file makes a reference to another one. Nevertheless, it would be very verbose and painful to load each of the ~30 files from its corresponding \ID{ModelRessource}. Moreover, with our former approach we would need to add as many configuration parameters to the annotator as the number of resources it uses to make it completely independent of any version of its resources. Hence, considering a batch of files simply as a collection of resource files is not optimal.

We see two other alternatives:

\begin{enumerate}

\item First, if we devise a strict naming convention for each \ID{ModelResource} class involved in setting up an annotator, then we can ask the user for a unique package name -- instead of a potentially numerous \ID{ModelResource} class names -- and load each resource class from that package. While this solution is simple on the paper it actually requires as much refactoring as the idea elaborated in Section \ref{sec:packaging1}. Additionally, it doesn't solve the interdependency between resources illustrated above with \ID{LinnaeusAnnotator}.

\item Alternatively, if we consider that all the files used to configure an engine have names following a precise naming specification, then we could aggregate them in an archive that would get extracted at runtime in a temporary directory. This solution has at least two significant advantages:

\begin{itemize}

\item it requires only a minimal rewrite of the current Java code to extract and use the data,

\item and it allows files to refer to one another through relative paths.

\end{itemize}

\end{enumerate}

A bluima user such as Sherlok could specify which temporary directory should be used instead of a default one -- say, \PATH{/tmp} -- through an extra configuration parameter so that he can manage the lifetime of the extracted data. However, with or without this extra parameter, the system would make things relatively complicated for the system administrator. For example, when installing annotation tools the user cannot know beforehand how much disk space is needed.

\TODO{explain that not knowing the amount of space required is ok for Sherlok}

But more importantly this would imply adding a relatively complex version checking system in bluima: in order to not extract when it is already available to prevent degrading performance, we would have to check if the data is actually correct. This can be quite complex when, for example, upgrading the resources. And this is without mentioning having several pipelines using the same engine but with different version of its resources, nor dealing with multiple concurrent annotation instances. Finally, if we take the example of Sherlok running on several servers, we have the issue of data accessibility and replication for cache purposes.

\subsubsection{Flexibility}

In addition to all the above issues, we could argue that we actually remove flexibility from the user by forcing him to use a Java system to archive and manage resources despite the fact that the system using bluima could be Java agnostic from the final user point of view. It could actually be quite painful for someone who is using a source control management (SCM) tool, such as Git, to manage two distinct files structures: one that is convenient to the user and one for our jar archiving system.

Moreover, at the end of the day, we have extracted some data that the user had to package initially in order to make the installation process easy.

\subsubsection{Version E: A Better Alternative}

\TODO{find a better title...}

We argue that the installation process could have been much simpler: instead of all those intermediary steps the user could have simply installed the configuration files on a directory of his own. From the bluima's perspective, since this simpler approach does not enforce any versioning mechanism at all, the user is free to implement any system that matches his needs. Following from this fact, Sherlok can add an extra layer -- potentially directly integrated in the bundle configuration file -- to manage resources easily for the end user. Such system could (but is not restricted to) be based on Git: a user would define a (batch of) resource(s) with a Git repository and either a tag or a commit SHA and Sherlok would automatically download and install the specific version of the resources.

This last solution allies the flexibility of using Java \ID{File} objects -- and therefore involves only little refactoring of bluima annotators -- with the power of external, well-establish and powerful SCM tools in addition to avoid the limitation of having to rely solely on \ID{InputStream} to access resources.

\TODO{insert comparative table}

\subsection{Restructuring bluima}

\TODO{write me}

\TODO{Git submodules for resources for bluima }

\subsection{Pipeline Configuration in Sherlok}

\TODO{write me}

\TODO{git dependency for resources for sherlok, but not necessarily git submodules }

\subsubsection{Configuration Variables}

\TODO{write me}

\section{Future Work}

\TODO{ideas? support for more variable url pattern (svn, ...) }
\TODO{overview figure, e.g.  Sherlok's Linnaeus bundle, that downloads the bluima Maven jar (that itself depends on the Linnaeus Maven library) and the Linnaeus config file; Sherlok's Linnaeus pipeline that uses them to annotate species}

\begin{thebibliography}{99}

\bibitem{bbp}
    Blue Brain Project,
    \href{http://bluebrain.epfl.ch/}{bluebrain.epfl.ch}

\bibitem{sherlok}
    Sherlok,
    \href{http://sherlok.io}{sherlok.io}

\bibitem{bluima}
    bluima,
    \href{https://github.com/BlueBrain/bluima}{github.com/BlueBrain/bluima}

\bibitem{uima}
    Apache UIMA,
    \href{https://uima.apache.org/}{uima.apache.org}

\bibitem{ruta}
    Apache RUTA,
    \href{https://uima.apache.org/ruta.html}{uima.apache.org/ruta.html}

\bibitem{apachefundation}
    The Apache Software Foundation,
    \href{http://www.apache.org/}{apache.org}

\bibitem{opennlp}
    Apache OpenNLP,
    \href{https://opennlp.apache.org/}{opennlp.apache.org}

\bibitem{linnaeus}
    Linnaeus,
    \href{http://linnaeus.sourceforge.net/}{linnaeus.sourceforge.net}

\end{thebibliography}

\begin{appendix}
  \listoffigures
  \listoftables
\end{appendix}

\end{document}
